{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from https://github.com/jaywalnut310/glow-tts \"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "import pflow.utils as utils\n",
    "from pflow.utils.model import sequence_mask\n",
    "from pflow.models.components import commons\n",
    "\n",
    "log = utils.get_pylogger(__name__)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(channels))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_dims = len(x.shape)\n",
    "        mean = torch.mean(x, 1, keepdim=True)\n",
    "        variance = torch.mean((x - mean) ** 2, 1, keepdim=True)\n",
    "\n",
    "        x = (x - mean) * torch.rsqrt(variance + self.eps)\n",
    "\n",
    "        shape = [1, -1] + [1] * (n_dims - 2)\n",
    "        x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvReluNorm(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers = torch.nn.ModuleList()\n",
    "        self.conv_layers.append(torch.nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2))\n",
    "        self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.relu_drop = torch.nn.Sequential(torch.nn.ReLU(), torch.nn.Dropout(p_dropout))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                torch.nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size // 2)\n",
    "            )\n",
    "            self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.proj = torch.nn.Conv1d(hidden_channels, out_channels, 1)\n",
    "        self.proj.weight.data.zero_()\n",
    "        self.proj.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x_org = x\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x * x_mask)\n",
    "            x = self.norm_layers[i](x)\n",
    "            x = self.relu_drop(x)\n",
    "        x = x_org + self.proj(x)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class DurationPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, filter_channels, kernel_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "        self.conv_1 = torch.nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.norm_1 = LayerNorm(filter_channels)\n",
    "        self.conv_2 = torch.nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.norm_2 = LayerNorm(filter_channels)\n",
    "        self.proj = torch.nn.Conv1d(filter_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.conv_1(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.norm_1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv_2(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.norm_2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.proj(x * x_mask)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    ## RoPE module\n",
    "\n",
    "    Rotary encoding transforms pairs of features by rotating in the 2D plane.\n",
    "    That is, it organizes the $d$ features as $\\frac{d}{2}$ pairs.\n",
    "    Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it\n",
    "    by an angle depending on the position of the token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, base: int = 10_000):\n",
    "        r\"\"\"\n",
    "        * `d` is the number of features $d$\n",
    "        * `base` is the constant used for calculating $\\Theta$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = base\n",
    "        self.d = int(d)\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def _build_cache(self, x: torch.Tensor):\n",
    "        r\"\"\"\n",
    "        Cache $\\cos$ and $\\sin$ values\n",
    "        \"\"\"\n",
    "        # Return if cache is already built\n",
    "        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "            return\n",
    "\n",
    "        # Get sequence length\n",
    "        seq_len = x.shape[0]\n",
    "\n",
    "        # $\\Theta = {\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "        theta = 1.0 / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
    "\n",
    "        # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
    "\n",
    "        # Calculate the product of position index and $\\theta_i$\n",
    "        idx_theta = torch.einsum(\"n,d->nd\", seq_idx, theta)\n",
    "\n",
    "        # Concatenate so that for row $m$ we have\n",
    "        # $[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}]$\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "\n",
    "        # Cache them\n",
    "        self.cos_cached = idx_theta2.cos()[:, None, None, :]\n",
    "        self.sin_cached = idx_theta2.sin()[:, None, None, :]\n",
    "\n",
    "    def _neg_half(self, x: torch.Tensor):\n",
    "        # $\\frac{d}{2}$\n",
    "        d_2 = self.d // 2\n",
    "\n",
    "        # Calculate $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
    "        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n",
    "        \"\"\"\n",
    "        # Cache $\\cos$ and $\\sin$ values\n",
    "        x = rearrange(x, \"b h t d -> t b h d\")\n",
    "\n",
    "        self._build_cache(x)\n",
    "\n",
    "        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n",
    "        x_rope, x_pass = x[..., : self.d], x[..., self.d :]\n",
    "\n",
    "        # Calculate\n",
    "        # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
    "        neg_half_x = self._neg_half(x_rope)\n",
    "\n",
    "        x_rope = (x_rope * self.cos_cached[: x.shape[0]]) + (neg_half_x * self.sin_cached[: x.shape[0]])\n",
    "\n",
    "        return rearrange(torch.cat((x_rope, x_pass), dim=-1), \"t b h d -> b h t d\")\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        out_channels,\n",
    "        n_heads,\n",
    "        heads_share=True,\n",
    "        p_dropout=0.0,\n",
    "        proximal_bias=False,\n",
    "        proximal_init=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert channels % n_heads == 0\n",
    "\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.heads_share = heads_share\n",
    "        self.proximal_bias = proximal_bias\n",
    "        self.p_dropout = p_dropout\n",
    "        self.attn = None\n",
    "\n",
    "        self.k_channels = channels // n_heads\n",
    "        self.conv_q = torch.nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_k = torch.nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_v = torch.nn.Conv1d(channels, channels, 1)\n",
    "\n",
    "        # from https://nn.labml.ai/transformers/rope/index.html\n",
    "        self.query_rotary_pe = RotaryPositionalEmbeddings(self.k_channels * 0.5)\n",
    "        self.key_rotary_pe = RotaryPositionalEmbeddings(self.k_channels * 0.5)\n",
    "\n",
    "        self.conv_o = torch.nn.Conv1d(channels, out_channels, 1)\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        if proximal_init:\n",
    "            self.conv_k.weight.data.copy_(self.conv_q.weight.data)\n",
    "            self.conv_k.bias.data.copy_(self.conv_q.bias.data)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "\n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "\n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2))\n",
    "        query = rearrange(query, \"b (h c) t-> b h t c\", h=self.n_heads)\n",
    "        key = rearrange(key, \"b (h c) t-> b h t c\", h=self.n_heads)\n",
    "        value = rearrange(value, \"b (h c) t-> b h t c\", h=self.n_heads)\n",
    "\n",
    "        query = self.query_rotary_pe(query)\n",
    "        key = self.key_rotary_pe(key)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.k_channels)\n",
    "\n",
    "        if self.proximal_bias:\n",
    "            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n",
    "            scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        p_attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        p_attn = self.drop(p_attn)\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t)\n",
    "        return output, p_attn\n",
    "\n",
    "    @staticmethod\n",
    "    def _attention_bias_proximal(length):\n",
    "        r = torch.arange(length, dtype=torch.float32)\n",
    "        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n",
    "        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv_2 = torch.nn.Conv1d(filter_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.conv_1(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv_2(x * x_mask)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size=1,\n",
    "        p_dropout=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "        self.attn_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers_1 = torch.nn.ModuleList()\n",
    "        self.ffn_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers_2 = torch.nn.ModuleList()\n",
    "        for _ in range(self.n_layers):\n",
    "            self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))\n",
    "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "            self.ffn_layers.append(\n",
    "                FFN(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    filter_channels,\n",
    "                    kernel_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "        for i in range(self.n_layers):\n",
    "            x = x * x_mask\n",
    "            y = self.attn_layers[i](x, x, attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_1[i](x + y)\n",
    "            y = self.ffn_layers[i](x, x_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_2[i](x + y)\n",
    "        x = x * x_mask\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size=1,\n",
    "        p_dropout=0.0,\n",
    "        proximal_bias=False,\n",
    "        proximal_init=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.proximal_bias = proximal_bias\n",
    "        self.proximal_init = proximal_init\n",
    "\n",
    "        self.drop = nn.Dropout(p_dropout)\n",
    "        self.self_attn_layers = nn.ModuleList()\n",
    "        self.norm_layers_0 = nn.ModuleList()\n",
    "        self.encdec_attn_layers = nn.ModuleList()\n",
    "        self.norm_layers_1 = nn.ModuleList()\n",
    "        self.ffn_layers = nn.ModuleList()\n",
    "        self.norm_layers_2 = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.self_attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels, \n",
    "                    n_heads, \n",
    "                    p_dropout=p_dropout\n",
    "                    )\n",
    "                )\n",
    "            self.norm_layers_0.append(LayerNorm(hidden_channels))\n",
    "            self.encdec_attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels, \n",
    "                    n_heads, \n",
    "                    p_dropout=p_dropout\n",
    "                    )\n",
    "                )\n",
    "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "            self.ffn_layers.append(\n",
    "                FFN(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    filter_channels,\n",
    "                    kernel_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, x_mask, h, h_mask):\n",
    "        \"\"\"\n",
    "        x: decoder input\n",
    "        h: encoder output\n",
    "        \"\"\"\n",
    "        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n",
    "            device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "        x = x * x_mask\n",
    "        for i in range(self.n_layers):\n",
    "            y = self.self_attn_layers[i](x, x, self_attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_0[i](x + y)\n",
    "\n",
    "            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_1[i](x + y)\n",
    "\n",
    "            y = self.ffn_layers[i](x, x_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_2[i](x + y)\n",
    "        x = x * x_mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_type,\n",
    "        encoder_params,\n",
    "        duration_predictor_params,\n",
    "        n_vocab,\n",
    "        speech_in_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder_type = encoder_type\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_feats = encoder_params.n_feats\n",
    "        self.n_channels = encoder_params.n_channels\n",
    "\n",
    "        self.emb = torch.nn.Embedding(n_vocab, self.n_channels)\n",
    "        torch.nn.init.normal_(self.emb.weight, 0.0, self.n_channels**-0.5)\n",
    "\n",
    "        self.speech_in_channels = speech_in_channels\n",
    "        self.speech_out_channels = self.n_channels\n",
    "        self.speech_prompt_proj = torch.nn.Conv1d(self.speech_in_channels, self.speech_out_channels, 1)\n",
    "        \n",
    "        self.prenet = ConvReluNorm(\n",
    "            self.n_channels,\n",
    "            self.n_channels,\n",
    "            self.n_channels,\n",
    "            kernel_size=5,\n",
    "            n_layers=3,\n",
    "            p_dropout=0.5,\n",
    "        )\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            encoder_params.n_channels,\n",
    "            encoder_params.filter_channels,\n",
    "            encoder_params.n_heads,\n",
    "            encoder_params.n_layers,\n",
    "            encoder_params.kernel_size,\n",
    "            encoder_params.p_dropout,\n",
    "        )\n",
    "\n",
    "        self.Decoder = Decoder(\n",
    "            encoder_params.n_channels,\n",
    "            encoder_params.filter_channels,\n",
    "            encoder_params.n_heads,\n",
    "            encoder_params.n_layers,\n",
    "            encoder_params.kernel_size,\n",
    "            encoder_params.p_dropout,\n",
    "        )\n",
    "\n",
    "        self.proj_m = torch.nn.Conv1d(self.n_channels, self.n_feats, 1)\n",
    "\n",
    "        self.proj_w = DurationPredictor(\n",
    "            self.n_channels,\n",
    "            duration_predictor_params.filter_channels_dp,\n",
    "            duration_predictor_params.kernel_size,\n",
    "            duration_predictor_params.p_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x, \n",
    "            x_lengths, \n",
    "            speech_prompt,\n",
    "            ):\n",
    "        \"\"\"Run forward pass to the transformer based encoder and duration predictor\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): text input\n",
    "                shape: (batch_size, max_text_length)\n",
    "            x_lengths (torch.Tensor): text input lengths\n",
    "                shape: (batch_size,)\n",
    "            speech_prompt (torch.Tensor): speech prompt input\n",
    "\n",
    "        Returns:\n",
    "            mu (torch.Tensor): average output of the encoder\n",
    "                shape: (batch_size, n_feats, max_text_length)\n",
    "            logw (torch.Tensor): log duration predicted by the duration predictor\n",
    "                shape: (batch_size, 1, max_text_length)\n",
    "            x_mask (torch.Tensor): mask for the text input\n",
    "                shape: (batch_size, 1, max_text_length)\n",
    "        \"\"\"\n",
    "        x = self.emb(x) * math.sqrt(self.n_channels)\n",
    "        x = torch.transpose(x, 1, -1)\n",
    "\n",
    "        speech_prompt = self.speech_prompt_proj(speech_prompt)\n",
    "        x = torch.cat([speech_prompt, x], dim=2)\n",
    "        x_speech_lengths = x_lengths + speech_prompt.size(2)\n",
    "        print(speech_prompt.shape, x.shape , \"3\")\n",
    "        x_speech_mask = torch.unsqueeze(sequence_mask(x_speech_lengths, x.size(2)), 1).to(x.dtype)      \n",
    "\n",
    "        x = self.prenet(x, x_speech_mask)\n",
    "        print(speech_prompt.shape, x.shape , \"4\")\n",
    "\n",
    "        # split speech prompt and text input\n",
    "        speech_prompt = x[:, :, :speech_prompt.size(2)]\n",
    "        x = x[:, :, speech_prompt.size(2):]\n",
    "        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)      \n",
    "        speech_mask = x_speech_lengths - x_lengths\n",
    "        speech_mask = torch.unsqueeze(sequence_mask(speech_mask, speech_prompt.size(2)), 1).to(x.dtype)\n",
    "        print(speech_prompt.shape, x.shape , speech_mask, \"5\")\n",
    "        x = self.encoder(x, x_mask)\n",
    "        print(speech_prompt.shape, x.shape , \"6\")\n",
    "        x = self.Decoder(x, x_mask, speech_prompt, speech_mask)\n",
    "        print(speech_prompt.shape, x.shape , \"7\")\n",
    "        mu = self.proj_m(x) * x_mask\n",
    "        print(speech_prompt.shape, x.shape , \"8\")\n",
    "        x_dp = torch.detach(x)\n",
    "        logw = self.proj_w(x_dp, x_mask)\n",
    "\n",
    "        return mu, logw, x_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DurationPredictorParams:\n",
    "    filter_channels_dp: int\n",
    "    kernel_size: int\n",
    "    p_dropout: float\n",
    "\n",
    "@dataclass\n",
    "class EncoderParams:\n",
    "    n_feats: int\n",
    "    n_channels: int\n",
    "    filter_channels: int\n",
    "    filter_channels_dp: int\n",
    "    n_heads: int\n",
    "    n_layers: int\n",
    "    kernel_size: int\n",
    "    p_dropout: float\n",
    "    spk_emb_dim: int\n",
    "    n_spks: int\n",
    "    prenet: bool\n",
    "\n",
    "# Example usage\n",
    "duration_predictor_params = DurationPredictorParams(\n",
    "    filter_channels_dp=256,\n",
    "    kernel_size=3,\n",
    "    p_dropout=0.1\n",
    ")\n",
    "\n",
    "encoder_params = EncoderParams(\n",
    "    n_feats=80,\n",
    "    n_channels=192,\n",
    "    filter_channels=768,\n",
    "    filter_channels_dp=256,\n",
    "    n_heads=2,\n",
    "    n_layers=6,\n",
    "    kernel_size=3,\n",
    "    p_dropout=0.1,\n",
    "    spk_emb_dim=64,\n",
    "    n_spks=1,\n",
    "    prenet=True\n",
    ")\n",
    "\n",
    "textencoder = TextEncoder(\n",
    "    \"RoPE Encoder\",\n",
    "    encoder_params,\n",
    "    duration_predictor_params,\n",
    "    n_vocab=100,\n",
    "    speech_in_channels=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 20]) 3\n",
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 20]) 4\n",
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 10]) tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]) 5\n",
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 10]) 6\n",
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 10]) 7\n",
      "torch.Size([2, 192, 10]) torch.Size([2, 192, 10]) 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 4.8987e-01, -1.4266e-02,  2.6257e-01,  ...,  3.0208e-02,\n",
       "            7.4142e-01,  3.7779e-01],\n",
       "          [ 1.6794e-01,  8.4094e-01,  4.2973e-01,  ...,  1.0987e+00,\n",
       "            7.6204e-01,  7.4787e-01],\n",
       "          [ 1.2965e+00,  1.3417e+00,  7.1178e-01,  ...,  7.9380e-01,\n",
       "            1.2251e+00,  9.2418e-01],\n",
       "          ...,\n",
       "          [ 1.2498e+00,  8.7821e-01,  7.1776e-01,  ...,  5.2050e-01,\n",
       "            5.0516e-01,  1.5358e-01],\n",
       "          [ 2.8755e-01,  8.6527e-02,  2.4341e-01,  ...,  5.8802e-01,\n",
       "            2.2435e-01,  3.2502e-01],\n",
       "          [-8.4165e-01, -1.0337e+00, -6.0544e-02,  ..., -1.4913e+00,\n",
       "           -9.0628e-01, -9.1042e-01]],\n",
       " \n",
       "         [[-1.7824e-02,  6.5967e-02,  4.1164e-01,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 3.9686e-01,  8.7978e-02,  4.6090e-01,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-7.4801e-02,  5.3388e-01,  4.3315e-02,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 1.1207e+00,  6.6327e-01,  6.5379e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 4.0976e-01,  9.0903e-01,  7.1532e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.6405e-01, -3.1988e-01,  5.0946e-04,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]]], grad_fn=<MulBackward0>),\n",
       " tensor([[[ 0.6020,  0.4774, -0.0495, -0.1531, -0.1912,  0.3349,  0.6231,\n",
       "           -0.5662,  0.1030,  0.3758]],\n",
       " \n",
       "         [[-0.7431,  0.4287,  0.3874, -0.3371, -0.3186,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000]]], grad_fn=<MulBackward0>),\n",
       " tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 100, (2, 10))\n",
    "x_lengths = torch.tensor([10, 5])\n",
    "\n",
    "speech_prompt = torch.randn(2, 80, 10)\n",
    "\n",
    "textencoder(x, x_lengths, speech_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import Conv1d, Linear\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, scale=1000):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n",
    "        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, encoder_hidden, residual_channels, dilation):\n",
    "        super().__init__()\n",
    "        self.dilated_conv = Conv1d(residual_channels, 2 * residual_channels, 3, padding=dilation, dilation=dilation)\n",
    "        self.diffusion_projection = Linear(residual_channels, residual_channels)\n",
    "        self.conditioner_projection = Conv1d(encoder_hidden, 2 * residual_channels, 1)\n",
    "        self.output_projection = Conv1d(residual_channels, 2 * residual_channels, 1)\n",
    "\n",
    "    def forward(self, x, conditioner, diffusion_step):\n",
    "        diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n",
    "        conditioner = self.conditioner_projection(conditioner)\n",
    "        y = x + diffusion_step\n",
    "\n",
    "        y = self.dilated_conv(y) + conditioner\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)\n",
    "\n",
    "        y = self.output_projection(y)\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        return (x + residual) / math.sqrt(2.0), skip\n",
    "\n",
    "\n",
    "class DiffSingerNet(nn.Module):\n",
    "    def __init__(self, in_dims=80, residual_channels=256, encoder_hidden=128, dilation_cycle_length=1, residual_layers=20, spk_emb_dim=192,\n",
    "                 pe_scale=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe_scale = pe_scale\n",
    "\n",
    "        self.input_projection = Conv1d(in_dims, residual_channels, 1)\n",
    "        self.time_pos_emb = SinusoidalPosEmb(residual_channels)\n",
    "        dim = residual_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            Mish(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.spk_mlp = nn.Sequential(\n",
    "            nn.Linear(spk_emb_dim, spk_emb_dim * 4),\n",
    "            Mish(),\n",
    "            nn.Linear(spk_emb_dim * 4, encoder_hidden)\n",
    "        )\n",
    "        self.residual_layers = nn.ModuleList([\n",
    "            ResidualBlock(encoder_hidden, residual_channels, 2 ** (i % dilation_cycle_length))\n",
    "            for i in range(residual_layers)\n",
    "        ])\n",
    "        self.skip_projection = Conv1d(residual_channels, residual_channels, 1)\n",
    "        self.output_projection = Conv1d(residual_channels, in_dims, 1)\n",
    "        nn.init.zeros_(self.output_projection.weight)\n",
    "\n",
    "    def forward(self, spec, spec_mask, mu, t, spk=None):\n",
    "        \"\"\"\n",
    "        :param spec: [B, M, T]\n",
    "        :param t: [B, ]\n",
    "        :param mu: [B, M, T]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # x = spec[:, 0]\n",
    "        x = spec\n",
    "        x = self.input_projection(x)  # x [B, residual_channel, T]\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        t = self.time_pos_emb(t, scale=self.pe_scale)\n",
    "        t = self.mlp(t)\n",
    "\n",
    "        if spk is not None:\n",
    "            s = self.spk_mlp(spk)\n",
    "            s = s.unsqueeze(-1).repeat(1, 1, x.shape[-1])\n",
    "            cond = s + mu\n",
    "        else:\n",
    "            cond = mu\n",
    "\n",
    "        skip = []\n",
    "        for layer_id, layer in enumerate(self.residual_layers):\n",
    "            x, skip_connection = layer(x, cond, t)\n",
    "            skip.append(skip_connection)\n",
    "\n",
    "        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n",
    "        x = self.skip_projection(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_projection(x)  # [B, M, T]\n",
    "        return x * spec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0334, -0.0334, -0.0334,  ..., -0.0334, -0.0334, -0.0334],\n",
       "         [-0.0564, -0.0564, -0.0564,  ..., -0.0564, -0.0564, -0.0564],\n",
       "         [ 0.0078,  0.0078,  0.0078,  ...,  0.0078,  0.0078,  0.0078],\n",
       "         ...,\n",
       "         [ 0.0239,  0.0239,  0.0239,  ...,  0.0239,  0.0239,  0.0239],\n",
       "         [-0.0561, -0.0561, -0.0561,  ..., -0.0561, -0.0561, -0.0561],\n",
       "         [ 0.0078,  0.0078,  0.0078,  ...,  0.0078,  0.0078,  0.0078]],\n",
       "\n",
       "        [[-0.0334, -0.0334, -0.0334,  ..., -0.0334, -0.0334, -0.0334],\n",
       "         [-0.0564, -0.0564, -0.0564,  ..., -0.0564, -0.0564, -0.0564],\n",
       "         [ 0.0078,  0.0078,  0.0078,  ...,  0.0078,  0.0078,  0.0078],\n",
       "         ...,\n",
       "         [ 0.0239,  0.0239,  0.0239,  ...,  0.0239,  0.0239,  0.0239],\n",
       "         [-0.0561, -0.0561, -0.0561,  ..., -0.0561, -0.0561, -0.0561],\n",
       "         [ 0.0078,  0.0078,  0.0078,  ...,  0.0078,  0.0078,  0.0078]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffnet = DiffSingerNet(encoder_hidden=80)\n",
    "\n",
    "x = torch.randn(2, 80, 10)\n",
    "x_mask = torch.ones(2, 1, 10)\n",
    "t = torch.tensor([4])\n",
    "mu = torch.randn(2, 80, 10)\n",
    "\n",
    "diffnet(x, x_mask, mu, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pflow.models.components import commons\n",
    "y = torch.randn(2, 80, 10)\n",
    "y_lengths = torch.tensor([10, 5])\n",
    "segment_size = 2\n",
    "z_slice, ids_slice = commons.rand_slice_segments(\n",
    "            y, y_lengths, segment_size\n",
    "        )\n",
    "ids_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mask = torch.unsqueeze(sequence_mask(y_lengths, y.size(2)), 1).to(y.dtype)\n",
    "y_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 0., 0., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 1., 1., 1., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(y.size(0)):  \n",
    "    y_mask[i,:,ids_slice[i]:ids_slice[i] + segment_size] = 0 \n",
    "y_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
