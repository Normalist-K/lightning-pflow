{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148f4bc0-c28e-4670-9a5e-4c7928ab8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 07:01:36.411991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 07:01:36.597607: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            150-230-187-81\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4126\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           150-230-187-81\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2023-11-17 07:01:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"0\"\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "from audiolm_pytorch import EncodecWrapper\n",
    "\n",
    "encodec = EncodecWrapper()\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e19ae5-a663-49fd-ac5f-7ffad4b53ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/LJSpeech/pflowtts_pytorch/logs/train/ljspeech/runs/2023-11-16_16-49-44/checkpoints/last.ckpt',\n",
       " '/home/ubuntu/LJSpeech/pflowtts_pytorch/logs/train/ljspeech/runs/2023-11-16_16-49-44/checkpoints/checkpoint_epoch=019.ckpt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(\"/home/ubuntu/LJSpeech/pflowtts_pytorch/logs/train/ljspeech/runs/2023-11-16_16-49-44/checkpoints/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5876c0-b47e-4c80-9e9c-62550f81b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pflow.models.pflow_tts import pflowTTS\n",
    "from pflow.text import sequence_to_text, text_to_sequence\n",
    "from pflow.utils.model import denormalize\n",
    "from pflow.utils.utils import get_user_data_dir, intersperse\n",
    "\n",
    "from pflow.hifigan.config import v1\n",
    "from pflow.hifigan.denoiser import Denoiser\n",
    "from pflow.hifigan.env import AttrDict\n",
    "from pflow.hifigan.models import Generator as HiFiGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a30306-588c-4f22-8d9b-e2676880b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# This allows for real time code changes being reflected in the notebook, no need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312856b-01a9-4d75-a4c8-4666dffa0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b3bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3f3db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/.local/share/pflow_tts')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_data_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3b3c3-d014-443b-84eb-e143cdec3e21",
   "metadata": {},
   "source": [
    "## Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7640a4c1-44ce-447c-a8ff-45012fb7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PFLOW_CHECKPOINT = \"/home/ubuntu/LJSpeech/pflowtts_pytorch/logs/train/ljspeech/runs/2023-11-16_16-49-44/checkpoints/checkpoint_epoch=019.ckpt\" #fill in the path to the pflow checkpoint\n",
    "HIFIGAN_CHECKPOINT = get_user_data_dir() / \"hifigan_T2_v1\"\n",
    "OUTPUT_FOLDER = \"synth_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477a3a9-71f2-4d2f-bb86-bdf3e31c2461",
   "metadata": {},
   "source": [
    "## Load TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a16230-04ba-4825-a844-2fb5ab945e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Parameter count: 33,515,009\n"
     ]
    }
   ],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    model = pflowTTS.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model\n",
    "count_params = lambda x: f\"{sum(p.numel() for p in x.parameters()):,}\"\n",
    "\n",
    "\n",
    "model = load_model(PFLOW_CHECKPOINT)\n",
    "print(f\"Model loaded! Parameter count: {count_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04dfadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocoder(checkpoint_path):\n",
    "    h = AttrDict(v1)\n",
    "    hifigan = HiFiGAN(h).to(device)\n",
    "    hifigan.load_state_dict(torch.load(checkpoint_path, map_location=device)['generator'])\n",
    "    _ = hifigan.eval()\n",
    "    hifigan.remove_weight_norm()\n",
    "    return hifigan\n",
    "\n",
    "# vocoder = load_vocoder(HIFIGAN_CHECKPOINT)\n",
    "# denoiser = Denoiser(vocoder, mode='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc2ba0-09ff-40e2-9e60-6b77b534f9fb",
   "metadata": {},
   "source": [
    "### Helper functions to synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "880a1879-24fd-4757-849c-850339120796",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def process_text(text: str):\n",
    "    x = torch.tensor(intersperse(text_to_sequence(text, ['english_cleaners2']), 0),dtype=torch.long, device=device)[None]\n",
    "    x_lengths = torch.tensor([x.shape[-1]],dtype=torch.long, device=device)\n",
    "    x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "    return {\n",
    "        'x_orig': text,\n",
    "        'x': x,\n",
    "        'x_lengths': x_lengths,\n",
    "        'x_phones': x_phones\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def synthesise(text, prompt):\n",
    "    text_processed = process_text(text)\n",
    "    start_t = dt.datetime.now()\n",
    "    output = model.synthesise(\n",
    "        text_processed['x'], \n",
    "        text_processed['x_lengths'],\n",
    "        n_timesteps=n_timesteps,\n",
    "        temperature=temperature,\n",
    "        length_scale=length_scale,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    # merge everything to one dict    \n",
    "    output.update({'start_t': start_t, **text_processed})\n",
    "    return output\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def to_waveform(mel, vocoder):\n",
    "#     audio = vocoder(mel).clamp(-1, 1)\n",
    "#     audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "#     return audio.cpu().squeeze()\n",
    "    \n",
    "# def save_to_folder(filename: str, output: dict, folder: str):\n",
    "#     folder = Path(folder)\n",
    "#     folder.mkdir(exist_ok=True, parents=True)\n",
    "#     np.save(folder / f'{filename}', output['mel'].cpu().numpy())\n",
    "#     sf.write(folder / f'{filename}.wav', output['waveform'], 22050, 'PCM_24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f857e3-2ef7-4c86-b776-596c4d3cf875",
   "metadata": {},
   "source": [
    "## Setup text to synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e0a9acd-0845-4192-ba09-b9683e28a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"text is text, one two three.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da9e2d-99b9-4c6f-8a08-c828e2cba121",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d216e5-4895-4da8-9d24-9e61021d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of ODE Solver steps\n",
    "n_timesteps = 10\n",
    "\n",
    "## Changes to the speaking rate\n",
    "length_scale=1.0\n",
    "\n",
    "## Sampling temperature\n",
    "temperature = 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feed5d13-c814-4f69-818f-6833d7f0d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import glob\n",
    "wav_files = glob.glob(\"/home/ubuntu/LJSpeech/LJSpeech-1.1/wavs/*.wav\")\n",
    "wav, sr = torchaudio.load(wav_files[0])\n",
    "mel = encodec(wav, curtail_from_left = True, return_encoded = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52a0cfea-7649-450e-8bb5-1707715ca2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "wav, sr = torchaudio.load(wav_files[0])\n",
    "from pflow.data.text_mel_datamodule import mel_spectrogram\n",
    "mel = mel_spectrogram(\n",
    "            wav,\n",
    "            1024,\n",
    "            80,\n",
    "            22050,\n",
    "            256,\n",
    "            1024,\n",
    "            0,\n",
    "            8000,\n",
    "            center=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2698323-2351-48ee-9d6e-a10b98c64d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = encodec(wav, curtail_from_left = True, return_encoded = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "279af7f5-14b7-4cbf-9018-13512909b5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 293, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69579c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = mel[0].transpose(1,2) #load a mel spectrogram from a file and paste it here; check dimensions [batch, channels, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "prompt = encodec.model.encoder(rearrange(wav, f'b t -> b {encodec.model.channels} t'))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aac89-c7f8-4975-8510-4e763c9689f4",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a227963-aa12-43b9-a706-1168b6fc0ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4755a56e1c4e4a8aad67e7cd91db156b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs, rtfs = [], []\n",
    "rtfs_w = []\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    prompt = prompt[:,:,:264]\n",
    "    output = synthesise(text, prompt) #, torch.tensor([15], device=device, dtype=torch.long).unsqueeze(0))\n",
    "    # output['waveform'] = to_waveform(output['mel'], vocoder)\n",
    "\n",
    "    # Compute Real Time Factor (RTF) with HiFi-GAN\n",
    "    # t = (dt.datetime.now() - output['start_t']).total_seconds()\n",
    "    # rtf_w = t * 22050 / (output['waveform'].shape[-1])\n",
    "\n",
    "    # ## Pretty print\n",
    "    # print(f\"{'*' * 53}\")\n",
    "    # print(f\"Input text - {i}\")\n",
    "    # print(f\"{'-' * 53}\")\n",
    "    # print(output['x_orig'])\n",
    "    # print(f\"{'*' * 53}\")\n",
    "    # print(f\"Phonetised text - {i}\")\n",
    "    # print(f\"{'-' * 53}\")\n",
    "    # print(output['x_phones'])\n",
    "    # print(f\"{'*' * 53}\")\n",
    "    # print(f\"RTF:\\t\\t{output['rtf']:.6f}\")\n",
    "    # print(f\"RTF Waveform:\\t{rtf_w:.6f}\")\n",
    "    # rtfs.append(output['rtf'])\n",
    "    # rtfs_w.append(rtf_w)\n",
    "\n",
    "    # Display the synthesised waveform\n",
    "    audio = encodec.decode(output['decoder_outputs'].transpose(-1,1))\n",
    "    ipd.display(ipd.Audio(audio.detach().cpu().numpy()[0][0], rate=22050))\n",
    "\n",
    "    ## Save the generated waveform\n",
    "#     save_to_folder(i, output, OUTPUT_FOLDER)\n",
    "\n",
    "# print(f\"Number of ODE steps: {n_timesteps}\")\n",
    "# print(f\"Mean RTF:\\t\\t\\t\\t{np.mean(rtfs):.6f} ± {np.std(rtfs):.6f}\")\n",
    "# print(f\"Mean RTF Waveform (incl. vocoder):\\t{np.mean(rtfs_w):.6f} ± {np.std(rtfs_w):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
